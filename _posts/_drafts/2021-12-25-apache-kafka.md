---
layout: post
title:  Apache Kafka
categories: [Apache Kafka,Messaging,PubSub,Event Driven,Data,Processing,Streaming]
excerpt: ... Another interpretion of this statement would be the importance of fullfilling the needs with the right application capabilities that turns data into most meaningful asset nowadays, aka information. Unless underpinned with right technology stack it is very hard to lay the foundation for a solid and sustainable enterprise architecture. As an event streaming platform evolved from a publish-subscribe messaging system, Apache Kafka manifests itself as a technology to collect, process, store and data at scale in a performant and reliable way.
---
[TOGAF<sup>&copy;</sup>](https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap12.html#tag_12_01) describes Technology Architecture as follows:
> Develop the Target Technology Architecture that enables the logical and physical application and data components and the Architecture Vision, addressing the Request for Architecture Work and stakeholder concerns.

Another interpretion of this statement would be the importance of fullfilling the needs with the right application capabilities that turns data into most meaningful asset nowadays, aka **information**. Unless underpinned with right technology stack it is very hard to shape a technology architecture that lays the foundation for enterprise architecture (EA). A solid and sustainable EA is the way to support the business vision and to achieve business outcomes that deliver value for customers so that for enterprise. 

As an event streaming platform evolved from a publish-subscribe messaging system, Apache Kafka manifests itself as a technology to collect, process, store the data at scale in a performant and reliable way[^1].

Before delving into Apache Kafka technology one needs to have a solid grasp on the relation between data and information, the concept of event, the drawbacks on client-server model and what event-driven systems bring to mitigate them.

# Data is Power
A business produces data at every stage of its lifetime while:
- Introducing a product
- Interacting with its customers
- Delivering value and services to its customers

The global data traffic increased 112 times from 2008 to 2020.[^2] To unleash the power of such a massive amount of data, it should be transformed into a more meaningful asset, information, using right combination of application and technology stack while specification, monitoring, execution of business processes. Because:
> "Every company is a technology company, regardless of what business they think they are in. A bank is just an IT company with a banking license." <br/>― _Christopher Little_  

This information is then used to gain **knowledge** in a quick and a straightforward way. It is the primary tool for understanding and overcoming business challenges in fast-paced and ever-changing world.

# Event
As stated in the introduction Apache Kafka is a streaming platform that persists and distribute **events**. An event denotes a change of a state, something happened in a domain or system and it is inherently a time bounded fact. As events are already happened, they are immutable and accumulation of these timed facts reveals **behavior** of the system or domain that they are generated by. The prevalence of interconnected and smart devices forces global businesses to capture and distill events carried by real-time digital data streams produced by them that mostly wipes out conventional commerce models. In order to gain meaningful insights and knowledge one should react to and process these events whilst storing them. In essence the enterprises/organizations that can process, enrich, transform, and respond to data incrementally as it is available become the leaders and pioneers in their business.     
 
# Client-Server Model (Request-Driven Systems)
During the rise of web era the common design pattern for systems domain was client-server model which is mainly backed by request-driven architecture which is synchronous and blocking in terms of communication and command execution. This is however, became inadequate to handle massive data volumes in a performant way. Because, the systems that embrace this model are tightly coupled. The requestor (client) and responder (server) need to know each other in order to interact each other. The need for such a seam between these actors turns out to be a maintenance and sustaining burden. As communication is synchronous, it leaves little room for error and there is no delivery guarantee in case of target system becomes unavailable, unless there exists a queuing, retrying mechanism in requestor side which contradicts to the simplicity of client implementation.

The variety of event sources (sensors on a plane, IoT devices on lorry fleet, click on an e-commerce site, etc.) also means that the systems that generate them may use different protocols, scaling strategies and error-handling mechanisms. The existence of myriad types of systems and software/technology specifications brings the maintenance and integration effort, so that cost, proportionally.

Generally speaking, these types of systems implements no control on the pace of incoming requests and data streams (e.g. ingress buffer for events). There is no focus on the context and content of data and what is being communicated. The last and may be the most prominent drawback is that the communication between client and server is not replayable. IOW, it is difficult to reconstruct or rollback the state in case of any need.

# Apache Kafka
> You do not need to leave your room. Remain sitting at your table and listen. Do not even listen, simply wait, be quiet, still and solitary. The world will freely offer itself to you to be unmasked.<br/>―  _Franz Kafka_, The Zürau Aphorisms

As one of the prominent writers of 20<sup>th</sup> century, Franz Kafka underlined the importance of listening to understand things happening around with aforementioned quote of him. Although Jay Kreps, co-creator of Apache Kafka, stated[^3] that to name a technology optimized for writing he used the name of a writer whom he likes, the above quote also overlaps with Apache Kafka's function of listening and processing events around.

## History
Apache Kafka built at LinkedIn in 2008 by Jay Kreps (technical lead of search systems at that time), Neha Narkhade and Jun Rao. The company open sourced the project in 2010 and it joined under Apache Umbrella in 2011. The team left the company in 2014[^4], and founded a new company named Confluent which provides enterprise event streaming solutions (on-premise and SaaS) on top of Apache Kafka technology. It is used by big tech unicorns like Netflix, Spotify and Uber[^5].

There existed two main challenges at LinkedIn that the team was asked to overcome:
1. Request/transaction monitoring system was faulty and worked with polling model:
    - Data points had large gaps.
    - Data model was not consistent.
    - Maintenance was not straightforward:
        - Schema changes had been turned out to be outage. 
        - Too much manual intervention needed.
2. Web backend servers streamed data to user activity tracking system using HTTP requests:
    - XML data collected and offloaded to an offline processing system.
    - No real-time insight was available (data processed in hourly batches).
    - Data from monitoring and activity tracking system could not be correlated easily:
        - There exists difference between data models where pull-push method became problematic. 

At first, ActiveMQ which is a popular traditional message broker was selected. Due to the middleware centric nature of this type of brokers while dispatching messages, it could not handle the data traffic that LinkedIn search engine encounters with. The flaws in that technology also caused broker instances grind to halt under heavy load.

After these bad experiences with ActiveMQ, they decided to implement a fit-for-purpose solution for LinkedIn. The technology needs to:
- Decouple data generators and users by using push-pull model
- Provide persistence for message data on messaging middleware with the ability to present data to multiple users
- Handle high data volume/throughput
- Scale up horizontally in case of need in proportion with data stream volume

## What Apache Kafka aims to resolve
Kafka is a distributed system of servers and clients that communicate using a performant TCP-based binary network protocol[^6] in an asynchronous manner. It can be installed on bare-metal servers, virtual machines, and containers in on-premise corporate data centers as well as on public and private cloud based platforms. It simply collects and stores data in a distributed commit log which is an ordered sequence of events/facts with the time of happening. This is in fact the state of a system or domain under observation at a specific time or time frame.

With that model Kafka, as a centralized communication hub (nervous system), simplifies communication and message interchange between systems. Systems can send and receive data with no need to know each other. It also embraces the famous publish-subscribe integration pattern[^7] to publish (write) to and subscribe (read) to streams of events with continuously importing/exporting data from other systems.

Kafka has the ability to streams of events durably and reliably as long as it is needed (with the boundary of storage limits) on distributed commit log that it manages. With that event store it is also possible to process streams of events as they happened or historically. Kafka brings distributed, highly scalable, elastic, fault-tolerant, and secure deployment model.

## Kafka as a messaging system
In its early days, Kafka appeared on the horizon as a messaging system. It is still a messaging system and also a streaming platform that stores data in distributed log files stored on persistent storage. These logs, in fact, are durable records of transactions. They provide a replay-able history and chain of events which can be used to re-build the state of a system at a certain time. Data is ordered and deterministically readable at partition (will be explained soon) level. Kafka is architected to work in cluster mode that is scalable, distributed and highly available. If any of nodes (server or instance that hosts Kafka binaries) in a cluster fails, the load is handed off to other nodes to achieve resiliency and continuity. This also prevents data loss, protects against failures and brings performance benefit. As a messaging middleware, it sits between systems and abstracts communication and integration between them. It can support large number of ad-hoc consumers with no dependency on type or use-case of them. Kafka makes use of consumers that can process large batches of data in timely or windowed fashion with the help of Kafka client API.     

## Basic Concepts
Apache Kafka runs in a cluster consists of processing nodes called as **brokers**. It organizes messages into **topics** which are logs per use-case/concern. **Producers** are the clients that push messages to broker. **Consumers** are the clients that pull messages from brokers. Topics are divided into **partitions** that shard messages across cluster according to the message keys of events. The event ordering is guaranteed <ins>through partitions</ins> not through topics. An event consists of **header**, **key**, **timestamp** and **value**. Kafka does not care about message formats and structures, as all of the data stored on Kafka is in bytes for the sake of performance. It is up to the clients to serialize and de-serialize messages into more meaningful structure using supported formats such as [Apache Avro](https://avro.apache.org), [Google ProtoBuf](https://developers.google.com/protocol-buffers/) and JSON.     

## Offsets
Kafka uses offsets as a unique identifier for messages they host on partitions. Offsets also denote the position of an ingested message in the partition. Consumers use offsets to define boundary of message (start-end indexes) stream that they need to consume. Kafka stores latest committed position by consumer to identify the next record that will be given out. Consumers can select automatic, periodic or manual committing. If a consumer app/process fails and restarts the consumer recovers the latest state by consuming events starting from latest committed offset on Kafka. Let's say if a consumer has a offset position committed at 8<sup>th</sup> index, that means it consumed records with offset 0 through 7 and will start to receive record stream beginning at offset 8 at the next iteration. Kafka writes messages in order to partitions indexed uniquely and sequentially via offsets. The address of a message on Kafka contains:
- The topic that hosts the message
- The partition that hosts the message
- The unique offset assigned to the message by Kafka

## Data Management
Kafka works on dumb broker-smart subscriber mode, which is why it does not care about the state of the consumer and just keeps data as an binary audit trail in distributed log files. Consumers need to know, store, build their state in case of any need. Consumers only communicate with partition leaders elected by Kafka for each topic.  
  
To make the cluster fault-tolerant and highly-available, and to keep data intact the content of partitions on topics can be copied across brokers (both within corporate datacenters and across geographically dispersed availability zones). With that replication scheme the ingested data always has a copy on multiple brokers for the sake of resilience, backup and avoid outages in case of failures and maintenance operations. The best practice for setting replication factor in production deployment is an odd number, 3 at minimum, to achieve quorum in an election process.

## Message Format
All messages on Kafka are stored/transmitted as raw bytes. It is required for performance in terms of high I/O throughput, less memory consumption and CPU cycles during data processing. Clients are responsible to serialize and de-serialize these byte streams to higher-level representations (e.g. String, Long, custom models) using protocol formats such as JSON, Avro, ProtoBuf. There is no message type or format checking on Kafka. Here is a list of external serializer/de-serializer (Serde) implementations that are needed for primitive types:

|Data Type|Serde|
|---------|-----|
|`byte[]`|`Serdes.ByteArray()`, `Serdes.Bytes()`|
|`ByteBuffer`|`Serdes.ByteBuffer()`|
|`Double`|`Serdes.Double()`|
|`Integer`|`Serdes.Integer()`|
|`Long`|`Serdes.Long()`|
|`String`|`Serdes.String()`|

## Consumer Groups
Consumer groups are set of consumers which cooperate to consume data in parallel. Partitions of all the topics are divided among the consumers in the group. Kafka automatically handles crashed consumers and re-assign previously assigned partitions to available consumers via re-balancing algorithm. Kafka elects a coordinator broker for each group to avoid anarchy (split brain problem) with managing members and partition assignments. The broker that hosts the leader of the partition number for a topic determined by the following formula is the group coordinator:
```math
hash(group_id) % (partition # of internal offset topic)
```
This calculation balances the load of consumer group management across cluster equally, so that the number of groups can be scaled up via increasing number of brokers. The internal offsets topic `__consumer_offsets`, is used to store committed offsets. When a consumer starts up it finds the coordinator and requests joining that group. Each member must send heartbeats to the coordinator. In case of timeout coordinator kicks off the consumer from group and re-assigns its partitions to another member in group.

[^1]: https://kafka.apache.org/intro#intro_platform
[^2]: https://www.foreignaffairs.com/articles/united-states/2021-04-16/data-power-new-rules-digital-age
[^3]: https://www.quora.com/What-is-the-relation-between-Kafka-the-writer-and-Apache-Kafka-the-distributed-messaging-system/answer/Jay-Kreps
[^4]: https://www.forbes.com/sites/stevenli1/2020/05/11/confluent-jay-kreps-kafka-4-billion-2020
[^5]: https://stackshare.io/kafka
[^6]: https://kafka.apache.org/protocol#protocol_network
[^7]: https://www.enterpriseintegrationpatterns.com/patterns/messaging/PublishSubscribeChannel.html